# Retrieval-Augmented Generation (RAG) System for Boeing 737 Service Documentation

### **Project Tagline**  
*A Contextual AI Solution for Accelerating Knowledge Retrieval and Decision Support for pilots.*

---

## Table of contents
1. [Introduction and Project Mandate](#1-introduction-and-project-mandate)  
2. [Architecture: The RAG Pipeline](#2-architecture-the-rag-pipeline)  
   - [2.1 The Indexing Phase (Corpus Preparation)](#21-the-indexing-phase-corpus-preparation)  
   - [2.2 The Retrieval Phase (Contextual Search)](#22-the-retrieval-phase-contextual-search)  
   - [2.3 The Generation Phase (Grounded Response)](#23-the-generation-phase-grounded-response)  
3. [File / Folder Overview](#3-file--folder-overview)  
4. [How to run (local, minimal)](#4-how-to-run-local-minimal)  
5. [Notes, configuration & best practices](#5-notes-configuration--best-practices)  

---

## 1. Introduction and Project Mandate

This repository implements a specialized **Retrieval-Augmented Generation (RAG)** system built to interact with the Boeing 737 service and maintenance documentation. The goal is to ground LLM outputs in verifiable technical data so that answers are **accurate**, **traceable**, and **audit-ready** — thereby reducing hallucinations and improving decision support for maintenance engineers and pilots.

---

## 2. Architecture: The RAG Pipeline

The system is organized as a three-stage pipeline: **Indexing → Retrieval → Generation**. The design focuses on multimodal grounding (text + page images / diagrams) and strong source attribution.

Project structure (summary):

```text
Rag-B737-Service-Repo/
├─ new_data/                      # generated by Preprocessing.py
│  ├─ page_images/                # page_*.png (full page screenshots)
│  ├─ manual_text.index           # FAISS/text index for text chunks
│  ├─ manual_diagram.index        # FAISS/index for diagram/context embeddings
│  ├─ manual.json                 # mapping: chunk_text, page_number, diagram_text, image path
├─ RAG_SERVICES/
│  ├─ model_wrapper.py            # embed query, load images, construct multimodal prompt, call model API
│  ├─ vector_store.py             # DualFAISSVectorStore (read indexes, cosine similarity search)
├─ Preprocessing.py               # generates new_data/ (ingest PDF, chunk, embed, extract diagram context)
├─ main.py                        # server entrypoint
├─ app.py                         # (optional) Streamlit interface
├─ .env.example                   # example env vars required (API keys, model names, etc.)
├─ requirements.txt               # python dependencies
```
### 2.1 The Indexing Phase (Corpus Preparation)

**Text-based indexing**

- **Document ingestion**: PDF loader (PyMuPDF) preserves page-level metadata used later for source attribution.

- **Chunking**: Fixed-size text chunks with overlap (configurable) to balance granularity vs. context.

- **Embedding**: Each chunk is embedded using the configured embedding model (example in repo uses Google embeddings).

- **Vector store persistence**: Vectors + metadata are saved in a performant vector store (FAISS in this implementation). A JSON mapping (*manual.json*) links chunk ↔ page ↔ image.

**Diagram / tables context indexing (multimodal)**

Diagrams, tables and technical illustrations lose semantic context when only text is embedded. The pipeline uses a VLM (Gemini VLM — *gemini-2.5-flash-preview-09-2025* in this project) to extract concise diagram context per page. This diagram text is embedded and stored in a separate diagram index.

Example (from Preprocessing.py) shows how image bytes are passed to the VLM and how the generated diagram description is used as diagram_text for the page.

Final mapping format (example):
```json
{
  "page_number": 12,
  "chunk_index": 3,
  "text": "chunk text ...",
  "image_path": "new_data/page_images/page_12.png",
  "diagram_text": "concise diagram description generated by VLM"
}
```
### 2.2 The Retrieval Phase (Contextual Search)

- **Query embedding**: User query is embedded using the same embedding model.

- **Dual-index search**: A similarity search runs on both the text index and the diagram index. Results are merged and weighted to produce a combined relevance score (see *vector_store.py* / *DualFAISSVectorStore*).

- **Context construction**: The top-k grounding chunks (text + associated page images) are assembled and passed to the LLM with strict instructions to use only those sources.

Example (merge logic excerpt):
```python
text_scores, text_idx = self.text_index.search(query, k)
diagram_scores, diagram_idx = self.diagram_index.search(query, k)
# results are merged with per-index scores, producing combined relevance
```
### 2.3 The Generation Phase (Grounded Response)

- **Augmented prompting**: Construct a multimodal prompt including system instruction, retrieved text chunks, and the full-page images (as inline base64 payloads), telling the model to answer only from the provided context.

- **LLM call**: A single API invocation (Gemini multimodal) returns the grounded answer.

- **Source attribution**: The response is returned with a compact "Relevant Pages: [..]" line and the server-side metadata contains the mapping to source chunk(s) so every claim is traceable.

Prompt construction follows the pattern in *model_wrapper.py* — system instruction + ordered context by page number + strict output format (Relevant Pages line).

## 3. File / Folder Overview (concise)

- **new_data/** — generated dataset: images, index files, mapping JSON that connects text chunks to pages and diagram descriptions.

- **RAG_SERVICES/model_wrapper.py** — handles embedding the user query, loading the page images, building the multimodal prompt, invoking the Gemini API, and post-processing the model answer.

- **RAG_SERVICES/vector_store.py** — DualFAISSVectorStore implementation: loads FAISS indices for text and diagram embeddings and provides search + merge logic.

- **Preprocessing.py** — ingestion and preprocessing script:

    - extract text and images from PDF (PyMuPDF),

    - chunk text + overlap,

    - call embedding model for text chunks,

    - call VLM to extract diagram descriptions (per-page) and embed them,

    - persist indexes and manual.json.

- **main.py** — server wrapper / API that accepts user queries, uses the vector store to retrieve context, calls model wrapper to generate responses, and returns answers plus provenance metadata.

- **app.py** — Streamlit demo UI (stream queries to main.py or call the service functions directly).

- **.env.example** — example environment variables (API keys, model names like VLM_MODEL, EMB_MODEL), follow it and create a local .env.


## 4. How to run (local, minimal)

- **Prerequisites**

    - **Python 3.9+** (or the version listed in requirements.txt)

    - **An API key for the multimodal LLM/VLM and for the embedding model (set in .env)**

Step-by-step (example):
```bash
# 1) create and activate a virtual environment (recommended)
python -m venv .venv
# linux / macOS
source .venv/bin/activate
# windows (powershell)
# .venv\Scripts\Activate.ps1

# 2) install dependencies
pip install -r requirements.txt

# 3) copy example env and edit
cp .env.example .env
# edit .env -> set API keys and model names (e.g., GEMINI_API_KEY, VLM_MODEL, EMB_MODEL, etc.)

# 4) generate the processed dataset (this creates new_data/)
python preprocessing.py

# 5) run the backend server
python main.py

# 6) run the Streamlit interface (in a new terminal with venv active)
streamlit run app.py

```

## 5. Notes, configuration & best practices

- **Environment variables**: Populate .env from .env.example. Typical required keys: model identifiers and API keys for embedding and multimodal models (VLM + text embeddings). The names used in the repo are the authoritative source — check .env.example.

- **Index rebuilds**: If you update the source PDF or change chunking/embedding parameters, re-run python preprocessing.py to regenerate new_data/.

- **Safety & strict grounding**: System-level prompts restrict the LLM to produce answers only from the provided context and require the exact Relevant Pages: [..] footer. Do not change this unless you understand the audit implications.


- **Performance**: FAISS indices are saved to disk for fast startup. Large manuals will require enough RAM for indexing/search and for loading images into prompts; consider batching or limiting top-k if you see memory pressure.

- **Extending indexes**: The vector store abstraction allows replacing FAISS by another vendor (Pinecone, Chroma, etc.) with minimal changes in *RAG_SERVICES/vector_store.py*.